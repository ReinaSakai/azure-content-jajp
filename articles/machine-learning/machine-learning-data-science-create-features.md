<properties
	pageTitle="Cortana Analytics Process の特徴エンジニアリング | Microsoft Azure" 
	description="特徴エンジニアリングの目的について説明し、機械学習のデータ強化プロセスにおけるその役割の例を示します。"
	services="machine-learning"
	documentationCenter=""
	authors="bradsev"
	manager="jhubbard"
	editor="cgronlun"/>

<tags
	ms.service="machine-learning"
	ms.workload="data-services"
	ms.tgt_pltfrm="na"
	ms.devlang="na"
	ms.topic="article"
	ms.date="09/19/2016"
	ms.author="zhangya;bradsev" />


# Cortana Analytics Process の特徴エンジニアリング 

このトピックでは、特徴エンジニアリングの目的について説明し、機械学習のデータ強化プロセスにおけるその役割の例を示します。このプロセスの説明に使用されている例は、Azure Machine Learning Studio から引用しています。

[AZURE.INCLUDE [cap-create-features-data-selector](../../includes/cap-create-features-selector.md)]

この**メニュー**は、多様な環境のデータの特徴を作成する方法が説明されたトピックにリンクされています。このタスクは、[Team Data Science Process (TDSP)](https://azure.microsoft.com/documentation/learning-paths/cortana-analytics-process/) の 1 ステップです。

特徴エンジニアリングでは、学習プロセスの促進に役立つ生データから特徴を作成して、学習アルゴリズムの予測力を高めることを試みます。特徴のエンジニアリングと選択は TDSP の一部です。TDSP の概要については、「[What is the Team Data Science Process? (Team Data Science Process について)](data-science-process-overview.md)」を参照してください 特徴エンジニアリングと特徴選択は、TDSP の**特徴の開発**ステップの一部です。

* **特徴エンジニアリング**: このプロセスは、データ内の既存の生の特徴から、関連する特徴を作成し、学習アルゴリズムの予測力を高めようとします。

* **特徴選択**: このプロセスは、トレーニング問題の次元を削減するために、元のデータの特徴のキーのサブセットを選択します。

通常、**特徴エンジニアリング**は追加の特徴を生成するために最初に適用され、その後、無関係な特徴、重複した特徴、関連性の高い特徴を排除するために**特徴選択**の手順が実行されます。

機械学習で使用されるトレーニング データは、多くの場合、収集された生データから特徴を抽出することで向上させることができます。手書き文字の画像の分類方法の学習において、エンジニアリングされた特徴の例は、生のビット分布データで構成されているビット密度マップの作成です。このマップは、生の分布を直接使用するよりも効率的に文字のエッジを検索できます。


[AZURE.INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]


## データから特徴を作成する - 特徴エンジニアリング

トレーニング データは、例 (行に格納されているレコードや観測) からなるマトリックスで構成され、それぞれが特徴のセット (列に格納されている変数やフィールド) を持っています。実験計画で指定された特徴は、データ内のパターンを特徴付けることが期待されます。生のデータ フィールドの多くは、モデルのトレーニングに使用される、選択された特徴セットに直接含めることができますが、多くの場合、追加の (エンジニアリングされた) 特徴は、強化されたトレーニング データセットを生成するために生データの特徴から構築される必要があります。

モデルをトレーニングする場合は、データセットを強化するためにどのような特徴を作成する必要がありますか。 トレーニングを強化するようにエンジニアリングされた特徴は、データ内のパターンをより適切に識別する情報を提供します。新しい特徴は、元の特徴セットや既存の特徴セットを明確に取り込んだり、簡単に表示したりしない追加情報を提供することが期待されます。ただし、このプロセスは芸術のようなものです。正当な生産性の高い意思決定は、多くの場合、専門知識が必要です。

まず Azure Machine Learning から始めると、Studio で提供されているサンプルを使用して最も簡単にこのプロセスを具体的に把握できます。次に、2 つの例を示します。

* ターゲット値が既知の場合の教師あり実験における、[レンタル自転車の数の予測](http://gallery.cortanaintelligence.com/Experiment/Regression-Demand-estimation-4)の回帰の例
* [特徴ハッシュ](https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/)を使用したテキスト マイニングの分類例

### 例 1: 回帰モデルに時間的な特徴を追加する ###

Azure Machine Learning Studio で ”自転車の需要予測” の実験を使用して、回帰タスクの特徴をエンジニアリングする方法を見てみましょう。この実験の目的は、自転車の需要、つまり特定の月 / 日 / 時間内でのレンタル自転車の数を予測することです。データセットの "Bike Rental UCI データセット" は、生の入力データとして使用します。このデータセットは、米国のワシントン D.C. で自転車のレンタル ネットワークを管理している Capital Bikeshare 社の実際のデータに基づいています。データセットは、2011 年と 2012 年の 1 日の特定の時間帯のレンタル自転車の数を表し、17379 行と 17 列が含まれています。生の特徴セットには、気象条件 (温度 / 湿度 / 風速) やその日の種類 (休日 / 平日) が含まれます。予測するフィールドは "cnt" です。これは、特定の時間帯のレンタル自転車数を 1 ～ 977 の範囲内で表しています。

トレーニング データに効果的な特徴を構築することを目標として、4 つの回帰モデルは同じアルゴリズムを使用して構築されましたが、トレーニング データセットはそれぞれ異なります。次の 4 つのデータセットは同じ生の入力データを表しますが、特徴セットの数は増加しています。これらの特徴は、次の 4 つのカテゴリに分類されます。

1. A = 予測日の天候 + 休日 + 平日 + 週末の各特徴
2. B = 過去 12 時間ごとにレンタルされた自転車の数
3. C = 過去 12 日ごとにレンタルされた自転車の数 (同じ時間帯)
4. D = 過去 12 週ごとにレンタルされた自転車の数 (同じ時間帯、同じ日)

元の生データに既に存在する特徴セット A の他に、他の 3 つの特徴セットが特徴エンジニアリング プロセスによって作成されます。特徴セット B では、直近の自転車の需要を収集します。特徴セット C では、特定の時間帯の自転車の需要を収集します。特徴セット D では、特定の曜日の特定の時間帯の自転車の需要を収集します。この 4 つのトレーニング データセットは、それぞれ A、A+B、A+B+C、A+B+C+D の特徴セットを含んでいます。

Azure Machine Learning の実験では、これら 4 つのトレーニング データセットは、前処理された入力データセットからの 4 つの分岐を使用して形成されます。一番左の分岐を除いて、これらの各分岐には [R スクリプトの実行](https://msdn.microsoft.com/library/azure/30806023-392b-42e0-94d6-6b775a6e0fd5/)モジュールが含まれており、生成された一連の特徴 (特徴セット B、C、D) は個別に構築され、インポートされたデータセットに追加されます。次の図は、左の 2 番目の分岐にある特徴セット B の作成に使用される R スクリプトを示します。

![特徴を定義する](./media/machine-learning-data-science-create-features/addFeature-Rscripts.png)

4 つのモデルのパフォーマンス結果の比較を次の表にまとめています。特徴 A+B+C よって最適な結果が表示されます。トレーニング データに特徴セットを追加すると、エラー率が低下する点に注意してください。これにより、特徴セット B、C は、関連する追加情報を回帰タスクに提供するという推測が検証されます。ただし、D の特徴を追加しても、エラー率はそれ以上は低下しないようです。

![結果の比較](./media/machine-learning-data-science-create-features/result1.png)

### <a name="example2"></a> 例 2: テキスト マイニングで特徴を作成する  

特徴エンジニアリングは、ドキュメントの分類やセンチメント分析などのテキスト マイニングに関連するタスクに広く適用されています。たとえば、ドキュメントをいくつかのカテゴリに分類する場合は、通常、1 つのドキュメントのカテゴリに含まれる単語や語句が別のドキュメントのカテゴリに存在する可能性が低いことを前提とします。つまり、単語や語句の配布の頻度は異なるドキュメント カテゴリを特徴付けることができます。テキスト マイニング アプリケーションでは、通常、テキスト コンテンツの各部分が入力データとして機能するため、出現する単語や語句の頻度を含む特徴を作成するために特徴エンジニアリング プロセスが必要になります。

このタスクを実現するには、**特徴ハッシュ**と呼ばれる手法を適用して、任意のテキストの特徴を効率的にインデックスに変えます。各テキストの特徴 (単語や語句) を特定のインデックスに関連付ける代わりに、ハッシュ関数を特徴に適用し、そのハッシュ値を直接インデックスとして使用することでこのメソッドが機能します。

Azure Machine Learning には、これらの単語や語句の特徴を都合よく作成する、[特徴ハッシュ](https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/) モジュールがあります。このモジュールを使用する例を次に示します。入力データセットには、1 ～ 5 の書籍の評価と実際のレビュー内容の 2 つ列が含まれています。この[特徴ハッシュ](https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/) モジュールの目的は、特定の書籍レビュー内の対応する単語や語句の出現頻度を示す一連の新しい特徴を取得することです。このモジュールを使用するには、次の手順を完了する必要があります。

* まず、入力テキストが含まれている列を選択します (この例では "Col2")。
* 次に、"Hashing bitsize" を 8 に設定します。これにより、2 ^8 = 256 の特徴が作成されます。すべてのテキストの単語や語句は、256 のインデックスにハッシュされます。"Hashing bitsize" パラメーターの範囲は 1 ～ 31 です。単語や語句は、大きな数値を設定すれば、同じインデックスにハッシュされる可能性はほとんどありません。
* 最後に、"N-grams" パラメーターを 2 に設定します。これは、入力テキストからユニグラム (単語ごとの特徴) とバイグラム (隣接する2 単語ごとの特徴) の出現頻度を取得します。"N-grams" パラメーターの範囲は 0 ～ 10 で、特徴に含まれる最大の連続単語数を示します。

![”特徴ハッシュ” モジュール](./media/machine-learning-data-science-create-features/feature-Hashing1.png)

次の図は、これらの新しい特徴の外観を示しています。

![”特徴ハッシュ” の例](./media/machine-learning-data-science-create-features/feature-Hashing2.png)


## まとめ

エンジニアリングされ、選択された特徴は、データに含まれるキー情報の抽出を試みるトレーニング プロセスの効率を高めます。また、入力データを正確に分類して、関心のある結果をより確実に予測するために、これらのモデルのパワーを向上させます。特徴エンジニアリングと特徴選択は、学習を計算的により扱いやすくするために組み合わせることもできます。これは、強化した後、モデルの調整やトレーニングに必要な特徴の数を減らすことによって行われます。数学的に言うと、モデルのトレーニングに選択される特徴は、データのパターンを説明し、正常に結果を予測する独立変数の最小セットです。

必ずしも特徴エンジニアリングや特徴選択を実行する必要はありません。必要があるかどうかは、持っているデータや収集するデータ、選択するアルゴリズム、実験の目的によって異なります。
 

<!---HONumber=AcomDC_0921_2016-->